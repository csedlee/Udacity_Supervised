{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Quiz for \"Mini-Batch Gradient Descent\"\n",
    "\n",
    "Mini-Batch Gradient Descent Quiz\n",
    "In this quiz, you'll be given the following sample dataset (as in data.csv), and your goal is to write a function that executes mini-batch gradient descent to find a best-fitting regression line. You might consider looking into numpy's matmul function for this!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"l2-gradient-descent-data.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting a random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# TODO: Fill in code in the function below to implement a gradient descent\n",
    "# step for linear regression, following a squared error rule. See the docstring\n",
    "# for parameters and returned variables.\n",
    "def MSEStep(X, y, W, b, learn_rate = 0.005):\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent step for squared error as a\n",
    "    performance metric.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    W : predictor feature coefficients\n",
    "    b : regression function intercept\n",
    "    learn_rate : learning rate\n",
    "\n",
    "    Returns\n",
    "    W_new : predictor feature coefficients following gradient descent step\n",
    "    b_new : intercept following gradient descent step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill in code\n",
    "    # compute errors\n",
    "    y_pred = np.matmul(X, W) + b\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # compute steps\n",
    "    W_new = W + learn_rate * np.matmul(error, X)\n",
    "    b_new = b + learn_rate * error.sum()\n",
    "    return W_new, b_new\n",
    "\n",
    "\n",
    "# The parts of the script below will be run when you press the \"Test Run\"\n",
    "# button. The gradient descent step will be performed multiple times on\n",
    "# the provided dataset, and the returned list of regression coefficients\n",
    "# will be plotted.\n",
    "def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):\n",
    "    \"\"\"\n",
    "    This function performs mini-batch gradient descent on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    batch_size : how many data points will be sampled for each iteration\n",
    "    learn_rate : learning rate\n",
    "    num_iter : number of batches used\n",
    "\n",
    "    Returns\n",
    "    regression_coef : array of slopes and intercepts generated by gradient\n",
    "      descent procedure\n",
    "    \"\"\"\n",
    "    n_points = X.shape[0]\n",
    "    W = np.zeros(X.shape[1]) # coefficients\n",
    "    b = 0 # intercept\n",
    "    \n",
    "    # run iterations\n",
    "    regression_coef = [np.hstack((W,b))]\n",
    "    for _ in range(num_iter):\n",
    "        batch = np.random.choice(range(n_points), batch_size)\n",
    "        X_batch = X[batch,:]\n",
    "        y_batch = y[batch]\n",
    "        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)\n",
    "        regression_coef.append(np.hstack((W,b)))\n",
    "    \n",
    "    return regression_coef\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # perform gradient descent\n",
    "    data = np.loadtxt('data.csv', delimiter = ',')\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    regression_coef = miniBatchGD(X, y)\n",
    "    \n",
    "    # plot the results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure()\n",
    "    X_min = X.min()\n",
    "    X_max = X.max()\n",
    "    counter = len(regression_coef)\n",
    "    for W, b in regression_coef:\n",
    "        counter -= 1\n",
    "        color = [1 - 0.92 ** counter for _ in range(3)]\n",
    "        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)\n",
    "    plt.scatter(X, y, zorder = 3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.72407,2.23863\n",
    "-2.40724,-0.00156\n",
    "2.64837,3.01665\n",
    "0.36092,2.31019\n",
    "0.67312,2.05950\n",
    "-0.45460,1.24736\n",
    "2.20168,2.82497\n",
    "1.15605,2.21802\n",
    "0.50694,1.43644\n",
    "-0.85952,1.74980\n",
    "-0.59970,1.63259\n",
    "1.46804,2.43461\n",
    "-1.05659,1.02226\n",
    "1.29177,3.11769\n",
    "-0.74565,0.81194\n",
    "0.15033,2.81910\n",
    "-1.49627,0.53105\n",
    "-0.72071,1.64845\n",
    "0.32924,1.91416\n",
    "-0.28053,2.11376\n",
    "-1.36115,1.70969\n",
    "0.74678,2.92253\n",
    "0.10621,3.29827\n",
    "0.03256,1.58565\n",
    "-0.98290,2.30455\n",
    "-1.15661,1.79169\n",
    "0.09024,1.54723\n",
    "-1.03816,1.06893\n",
    "-0.00604,1.78802\n",
    "0.16278,1.84746\n",
    "-0.69869,1.58732\n",
    "1.03857,1.94799\n",
    "-0.11783,3.09324\n",
    "-0.95409,1.86155\n",
    "-0.81839,1.88817\n",
    "-1.28802,1.39474\n",
    "0.62822,1.71526\n",
    "-2.29674,1.75695\n",
    "-0.85601,1.12981\n",
    "-1.75223,1.67000\n",
    "-1.19662,0.66711\n",
    "0.97781,3.11987\n",
    "-1.17110,0.56924\n",
    "0.15835,2.28231\n",
    "-0.58918,1.23798\n",
    "-1.79678,1.35803\n",
    "-0.95727,1.75579\n",
    "0.64556,1.91470\n",
    "0.24625,2.33029\n",
    "0.45917,3.25263\n",
    "1.21036,2.07602\n",
    "-0.60116,1.54254\n",
    "0.26851,2.79202\n",
    "0.49594,1.96178\n",
    "-2.67877,0.95898\n",
    "0.49402,1.96690\n",
    "1.18643,3.06144\n",
    "-0.17741,1.85984\n",
    "0.57938,1.82967\n",
    "-2.14926,0.62285\n",
    "2.27700,3.63838\n",
    "-1.05695,1.11807\n",
    "1.68288,2.91735\n",
    "-1.53513,1.99668\n",
    "0.00099,1.76149\n",
    "0.45520,2.31938\n",
    "-0.37855,0.90172\n",
    "1.35638,3.49432\n",
    "0.01763,1.87838\n",
    "2.21725,2.61171\n",
    "-0.44442,2.06623\n",
    "0.89583,3.04041\n",
    "1.30499,2.42824\n",
    "0.10883,0.63190\n",
    "1.79466,2.95265\n",
    "-0.00733,1.87546\n",
    "0.79862,3.44953\n",
    "-0.12353,1.53740\n",
    "-1.34999,1.59958\n",
    "-0.67825,1.57832\n",
    "-0.17901,1.73312\n",
    "0.12577,2.00244\n",
    "1.11943,2.08990\n",
    "-3.02296,1.09255\n",
    "0.64965,1.28183\n",
    "1.05994,2.32358\n",
    "0.53360,1.75136\n",
    "-0.73591,1.43076\n",
    "-0.09569,2.81376\n",
    "1.04694,2.56597\n",
    "0.46511,2.36401\n",
    "-0.75463,2.30161\n",
    "-0.94159,1.94500\n",
    "-0.09314,1.87619\n",
    "-0.98641,1.46602\n",
    "-0.92159,1.21538\n",
    "0.76953,2.39377\n",
    "0.03283,1.55730\n",
    "-1.07619,0.70874\n",
    "0.20174,1.76894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSEStep(X, y, W, b, learn_rate = 0.001):\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent step for squared error as a\n",
    "    performance metric.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    W : predictor feature coefficients\n",
    "    b : regression function intercept\n",
    "    learn_rate : learning rate\n",
    "\n",
    "    Returns\n",
    "    W_new : predictor feature coefficients following gradient descent step\n",
    "    b_new : intercept following gradient descent step\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute errors\n",
    "    y_pred = np.matmul(X, W) + b\n",
    "    error = y - y_pred\n",
    "    \n",
    "    # compute steps\n",
    "    W_new = W + learn_rate * np.matmul(error, X)\n",
    "    b_new = b + learn_rate * error.sum()\n",
    "    return W_new, b_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
